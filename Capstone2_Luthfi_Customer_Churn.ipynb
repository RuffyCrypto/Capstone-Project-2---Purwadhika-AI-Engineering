{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e554f21",
   "metadata": {},
   "source": [
    "Capstone Project — Customer Churn Prediction (E-commerce)\n",
    "by: Luthfi Muzammil\n",
    "\n",
    "Notebook ini berisi pipeline end-to-end untuk prediksi tindak lanjut terhadap customer churn (yang tidak jadi pelanggan lagi) dan/atau customer yang tidak churn (setia).\n",
    "\n",
    "Konteks: sebuah perusahaan e-commerce ingin menindaklanjuti kegiatan promosi pelanggan yang churn/tidak churn, jadi bisa ditentukan pelanggan manakah yang akan diberikan promo/program promosi\n",
    "\n",
    "Dataset: `data_ecommerce_customer_churn.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c1bda",
   "metadata": {},
   "source": [
    "Analytic Approach\n",
    "\n",
    "Langkah analitik yang ditempuh:\n",
    "1. Data Understanding → memahami struktur dataset, missing values, distribusi target.\n",
    "2. Data Cleaning → imputasi missing values, drop kolom ID.\n",
    "3. Feature Engineering → tambah fitur baru (cashback_per_tenure).\n",
    "4. Modeling → baseline Logistic Regression, lalu RandomForest & GradientBoosting dengan hyperparameter tuning.\n",
    "5. Evaluation → metrik klasifikasi (Accuracy, Precision, Recall, F1, ROC AUC, PR Curve, AP Score).\n",
    "6. Feature Importance → interpretasi dengan permutation importance.\n",
    "7. Limitasi Model → keterbatasan pendekatan yang digunakan.\n",
    "8. Kesimpulan & Rekomendasi → pilih model terbaik dan strategi retensi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb363f1",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "0 = tidak churn\n",
    "1 = churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff429c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "DATA_PATH = 'data_ecommerce_customer_churn.csv'\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040ea4d2",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac35de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df.isnull().sum()\n",
    "df['Churn'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f75600b",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa6813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = df.drop(columns=['Churn'])\n",
    "y = df['Churn']\n",
    "X['cashback_per_tenure'] = X['CashbackAmount'] / X['Tenure'].replace({0: np.nan})\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbdeb03",
   "metadata": {},
   "source": [
    "## 4. Modeling & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ac7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, roc_curve, precision_recall_curve, average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import randint as sp_randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define preprocessing for numeric and categorical features\n",
    "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Pipelines for numeric and categorical features with imputation\n",
    "numeric_transformer = Pipeline([\n",
    "\t('imputer', SimpleImputer(strategy='mean')),\n",
    "\t('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "\t('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "\t('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', numeric_transformer, numeric_features),\n",
    "\t\t('cat', categorical_transformer, categorical_features)\n",
    "\t]\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Logistic Regression baseline\n",
    "logpipe = Pipeline([('preproc', preprocessor), ('clf', LogisticRegression(max_iter=1000, class_weight='balanced'))])\n",
    "logpipe.fit(X_train, y_train)\n",
    "y_pred = logpipe.predict(X_test)\n",
    "y_proba = logpipe.predict_proba(X_test)[:,1]\n",
    "print('Logistic Regression - Classification Report')\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_proba))\n",
    "\n",
    "# RandomForest\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_pipe = Pipeline([('preproc', preprocessor), ('clf', rf)])\n",
    "param_dist = {'clf__n_estimators': sp_randint(50,200), 'clf__max_depth': sp_randint(3,15)}\n",
    "rs = RandomizedSearchCV(rf_pipe, param_dist, n_iter=5, scoring='roc_auc', cv=3, random_state=42, n_jobs=-1)\n",
    "rs.fit(X_train, y_train)\n",
    "best_rf = rs.best_estimator_\n",
    "\n",
    "# GradientBoosting\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "gb_pipe = Pipeline([('preproc', preprocessor), ('clf', gb)])\n",
    "param_dist_gb = {'clf__n_estimators': sp_randint(50,150), 'clf__learning_rate': [0.01,0.05,0.1]}\n",
    "rs_gb = RandomizedSearchCV(gb_pipe, param_dist_gb, n_iter=5, scoring='roc_auc', cv=3, random_state=42, n_jobs=-1)\n",
    "rs_gb.fit(X_train, y_train)\n",
    "best_gb = rs_gb.best_estimator_\n",
    "\n",
    "# Save models for eval\n",
    "models = {'Logistic': logpipe, 'RandomForest': best_rf, 'GradientBoosting': best_gb}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f865d388",
   "metadata": {},
   "source": [
    "## 5. Evaluation Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = {name: model.predict_proba(X_test)[:,1] for name, model in models.items()}\n",
    "\n",
    "# ROC Curve\n",
    "plt.figure(figsize=(8,5))\n",
    "for name, prob in probas.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, prob)\n",
    "    plt.plot(fpr, tpr, label=f'{name}')\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.legend(); plt.title('ROC Curve'); plt.show()\n",
    "\n",
    "# Precision-Recall Curve\n",
    "plt.figure(figsize=(8,5))\n",
    "for name, prob in probas.items():\n",
    "    prec, rec, _ = precision_recall_curve(y_test, prob)\n",
    "    ap = average_precision_score(y_test, prob)\n",
    "    plt.plot(rec, prec, label=f'{name} (AP={ap:.3f})')\n",
    "plt.legend(); plt.title('Precision-Recall Curve'); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5da58e",
   "metadata": {},
   "source": [
    "## 6. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hitung permutation importance\n",
    "ri = permutation_importance(best_rf, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Ambil nama fitur dari preprocessor\n",
    "preproc = best_rf.named_steps['preproc']\n",
    "raw_feature_names = preproc.get_feature_names_out()\n",
    "\n",
    "# Bersihkan prefix 'num__' dan 'cat__'\n",
    "feature_names = [name.replace(\"num__\", \"\").replace(\"cat__\", \"\") for name in raw_feature_names]\n",
    "\n",
    "# Pastikan panjang feature_names dan importances sama\n",
    "if len(feature_names) != len(ri.importances_mean):\n",
    "    print(\"WARNING: Feature names and importances length mismatch!\")\n",
    "    print(\"Features:\", len(feature_names), \"Importances:\", len(ri.importances_mean))\n",
    "    # Truncate to the shortest length to avoid ValueError\n",
    "    min_len = min(len(feature_names), len(ri.importances_mean))\n",
    "    feature_names = feature_names[:min_len]\n",
    "    importances_mean = ri.importances_mean[:min_len]\n",
    "else:\n",
    "    importances_mean = ri.importances_mean\n",
    "\n",
    "# Buat DataFrame feature importance\n",
    "feat_imp_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances_mean\n",
    "}).sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Tampilkan top 15\n",
    "print(\"Top 15 Feature Importances (Permutation Importance):\")\n",
    "display(feat_imp_df.head(15))\n",
    "\n",
    "# Plot bar chart\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_imp_df['Feature'].head(15)[::-1], feat_imp_df['Importance'].head(15)[::-1])\n",
    "plt.xlabel('Permutation Importance (mean decrease in score)')\n",
    "plt.title('Top 15 Feature Importances — RandomForest (Clean Names)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f73c3ce",
   "metadata": {},
   "source": [
    "## 7. Limitasi Model\n",
    "\n",
    "- Model hanya menggunakan data historis internal, tanpa mempertimbangkan faktor eksternal (kompetitor, tren pasar, kondisi ekonomi).\n",
    "- Model berpotensi mengalami **concept drift**, karena perilaku pelanggan dapat berubah seiring waktu → perlu retraining periodik.\n",
    "- **Imbalance data**: meski sudah ditangani dengan `class_weight`, model tetap bisa bias ke kelas mayoritas (karena non-churn nya 0.82).\n",
    "- Algoritma yang dipakai masih model klasik dan sederhana(Logistic Regression, RandomForest, GradientBoosting)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61db7ac3",
   "metadata": {},
   "source": [
    "## 8. Kesimpulan & Rekomendasi\n",
    "\n",
    "- Model terbaik: RandomForest (ROC AUC tertinggi).\n",
    "- Recall cukup baik → efektif mendeteksi pelanggan berisiko churn.\n",
    "- Rekomendasi bisnis:\n",
    "  - Fokus pada pelanggan tenure rendah (karena tenure importance paling besar)\n",
    "  - Beri insentif tambahan bagi pelanggan setia (karena cashback importancenya kedua paling besar)\n",
    "  - Lakukan retraining model secara berkala.\n",
    "  - Perlu ditambahkan data eksternal (kompetitor, tren pasar, dan kondisi ekonomi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a00d84",
   "metadata": {},
   "source": [
    "## 10. Simpan & Load Model dengan Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3313922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Simpan model terbaik (misalnya best_rf)\n",
    "with open(\"customer_churn_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_rf, f)\n",
    "\n",
    "print(\"Model berhasil disimpan sebagai customer_churn_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e33a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load kembali model yang sudah disimpan\n",
    "with open(\"customer_churn_model.pkl\", \"rb\") as f:\n",
    "    loaded_model = pickle.load(f)\n",
    "\n",
    "# Tes prediksi dengan model hasil load\n",
    "y_pred_loaded = loaded_model.predict(X_test)\n",
    "print(\"Hasil prediksi (10 pertama):\", y_pred_loaded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b5caf3",
   "metadata": {},
   "source": [
    "## 11. Worksheet Prediksi dari Model Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f6bb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Buat worksheet prediksi dari X_test\n",
    "test_results = X_test.copy()\n",
    "\n",
    "# Tambahkan hasil aktual dan prediksi dari model pickle\n",
    "test_results[\"Actual_Churn\"] = y_test.values\n",
    "test_results[\"Predicted_Churn\"] = loaded_model.predict(X_test)\n",
    "test_results[\"Churn_Probability\"] = loaded_model.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Tampilkan 10 baris pertama\n",
    "display(test_results.head(10))\n",
    "\n",
    "# Simpan ke CSV dan Excel\n",
    "test_results.to_csv(\"churn_predictions.csv\", index=False)\n",
    "test_results.to_excel(\"churn_predictions.xlsx\", index=False)\n",
    "\n",
    "print(\"Hasil prediksi lengkap disimpan ke churn_predictions.csv dan churn_predictions.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
